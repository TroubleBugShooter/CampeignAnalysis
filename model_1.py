# -*- coding: utf-8 -*-
"""Model_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1drEXgA4AWarIL3npl--uepYhaianufUC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import r2_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.pipeline import Pipeline  # Import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
pd.set_option('display.max_columns', None)

label_df=pd.read_excel(r"label_df.xlsx")
df=pd.read_excel(r'processed_data.xlsx')

label_df

#Plotting countplot of clusters
pal = ["#682F2F","#B9C0C9", "#9F8A78","#F3AB60"]
pl = sns.countplot(x=label_df["label"], palette= pal)
pl.set_title("Arrangement Of The Clusters")
plt.show()

label_df.columns

df

df.columns

"""**Splitting the dataset into features and target**"""

# Step 4: Define Features (X) and Target (y)
X = label_df.drop(columns=['label','Wines', 'Fruits', 'Meat', 'Fish', 'Sweet', 'Gold',])
y = label_df['label']

# Step 5: Normalize Numerical Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 6: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

"""# Feture selection"""

# Initialize and fit Random Forest Classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Calculate Feature Importance
feature_importances = model.feature_importances_
feature_names = X.columns

# Create a DataFrame for better visualization
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

print(importance_df)

# Plot Feature Importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
plt.gca().invert_yaxis()  # Invert y-axis for better readability
plt.show()

importance_df['Cumulative'] = importance_df['Importance'].cumsum()
selected_features = importance_df[importance_df['Cumulative'] <= 0.95]['Feature'].tolist()
selected_features

imp_features=df[['Total_Spends',
 'Catalog_Purchase',
 'Income',
 'Store_Purchase',
 'Web_Purchase',
 'WebVisits_Month',
 'Kids',
 'Deal_Purchase_Discount',
 'Teens',
 'Recency']]

# Step 4: Define Features (X) and Target (y)
X = imp_features
y = label_df['label']

# Step 5: Normalize Numerical Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 6: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

"""# Random Forest"""

# Step 7: Train a Classification Model
RFmodel = RandomForestClassifier(random_state=42)
RFmodel.fit(X_train, y_train)

# Step 8: Evaluate the Model
y_RFpred = RFmodel.predict(X_test)
y_RFproba = RFmodel.predict_proba(X_test)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_RFpred))
print("\nClassification Report:\n", classification_report(y_test, y_RFpred))
print("\nROC-AUC Score:", roc_auc_score(y_test, y_RFproba))
print("R2 Score:", r2_score(y_test,y_RFpred))

"""## Hyperparameter Tuning- GridSearchCV"""

param_grid = {
    'n_estimators': [25, 50, 100, 150],
    'max_features': ['sqrt', 'log2', None],
    'max_depth': [3, 6, 9],
    'max_leaf_nodes': [3, 6, 9],
}

grid_search = GridSearchCV(RandomForestClassifier(),
                           param_grid=param_grid,)
grid_search.fit(X_train, y_train)
print(grid_search.best_estimator_)

# Step 7: Train a Classification Model
Tun_model = RandomForestClassifier(max_depth=9, max_features='log2', max_leaf_nodes=9,
                       n_estimators=50)
Tun_model.fit(X_train, y_train)

# Step 8: Evaluate the Model
y_RFpred = Tun_model.predict(X_test)
y_RFproba = Tun_model.predict_proba(X_test)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_RFpred))
print("\nClassification Report:\n", classification_report(y_test, y_RFpred))
print("\nROC-AUC Score:", roc_auc_score(y_test, y_RFproba))
print("R2 Score:", r2_score(y_test,y_RFpred))

"""# logical classification"""

# Initialize the Logistic Regression model
logicmodel = LogisticRegression()

# Train the model
logicmodel.fit(X_train, y_train)

# Make predictions
y_logicpred = logicmodel.predict(X_test)
y_logicproba = logicmodel.predict_proba(X_test)[:, 1]

# Evaluate the model
print("Confusion Matrix:\n", confusion_matrix(y_test, y_logicpred))
print("\nClassification Report:\n", classification_report(y_test, y_logicpred))
print("\nROC-AUC Score:", roc_auc_score(y_test, y_logicproba))
print("R2 Score:", r2_score(y_test, y_logicpred))

"""#"""

param_grid = [
    {'solver': ['saga'], 'penalty': ['l1', 'l2', 'elasticnet'], 'l1_ratio': [0.5],'C' : np.logspace(-4,4,20),'max_iter'  : [100,1000,2500,5000]},
    {'solver': ['lbfgs'], 'penalty': ['l2'],'C' : np.logspace(-4,4,20),'max_iter'  : [100,1000,2500,5000]},

    {'solver': ['liblinear'], 'penalty': ['l1', 'l2'],'C' : np.logspace(-4,4,20),'max_iter'  : [100,1000,2500,5000]}

]

from sklearn.model_selection import GridSearchCV
clf = GridSearchCV(logicmodel,param_grid = param_grid, cv = 3, error_score='raise', verbose=3,n_jobs=-1)
best_clf=clf.fit(X_train, y_train)
best_clf.best_estimator_

tun_logModel=LogisticRegression(C=0.615848211066026, l1_ratio=0.5, penalty='elasticnet',solver='saga')

# Train the model
tun_logModel.fit(X_train, y_train)

# Make predictions
y_logicpred = tun_logModel.predict(X_test)
y_logicproba = tun_logModel.predict_proba(X_test)[:, 1]

# Evaluate the model
print("Confusion Matrix:\n", confusion_matrix(y_test, y_logicpred))
print("\nClassification Report:\n", classification_report(y_test, y_logicpred))
print("\nROC-AUC Score:", roc_auc_score(y_test, y_logicproba))
print("R2 Score:", r2_score(y_test, y_logicpred))

"""# SVM

"""

from sklearn.svm import SVC
SVC_Model = SVC(kernel='linear',probability=True)
SVC_Model.fit(X_train, y_train)

# Make predictions
y_svcpred = SVC_Model.predict(X_test)
#y_svcproba = SVC_Model.predict_proba(X_test)[:, 1]
y_svcproba = SVC_Model.predict_proba(X_test)[:, 1]
# Evaluate the model
print("Confusion Matrix:\n", confusion_matrix(y_test, y_svcpred))
print("\nClassification Report:\n", classification_report(y_test, y_svcpred))
print("\nROC-AUC Score:", roc_auc_score(y_test, y_svcproba))
print("R2 Score:", r2_score(y_test, y_svcpred))

param_grid = {'C': [0.1, 1, 10, 100, 1000],
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],

              'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}

grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
# fitting the model for grid search
grid.fit(X_train, y_train)

# print best parameter after tuning
print(grid.best_params_)

# print how our model looks after hyper-parameter tuning
print(grid.best_estimator_)

SVCTun_Model = SVC(C=10, gamma=0.01, kernel='rbf',probability=True)
SVCTun_Model.fit(X_train, y_train)

SVCTun_Model_predictions = SVCTun_Model.predict(X_test)
yTun_svcproba = SVC_Model.predict_proba(X_test)[:, 1]

# Evaluate the model
print("Confusion Matrix:\n", confusion_matrix(y_test, SVCTun_Model_predictions))
print("\nClassification Report:\n", classification_report(y_test, SVCTun_Model_predictions))
print("\nROC-AUC Score:", roc_auc_score(y_test,yTun_svcproba))
print("R2 Score:", r2_score(y_test, SVCTun_Model_predictions))

"""# XGBoost"""

from xgboost import XGBClassifier
from hyperopt import STATUS_OK, Trials, fmin, hp, tpe
from sklearn.model_selection import cross_val_score

model_XGB = XGBClassifier()

model_XGB.fit(X_train, y_train)

y_XGBpred = model_XGB.predict(X_test)
#y_svcproba = SVC_Model.predict_proba(X_test)[:, 1]
y_XGBproba = model_XGB.predict_proba(X_test)[:, 1]
# Evaluate the model
print("Confusion Matrix:\n", confusion_matrix(y_test, y_XGBpred))
print("\nClassification Report:\n", classification_report(y_test, y_XGBpred))
print("\nROC-AUC Score:", roc_auc_score(y_test, y_XGBproba))
print("R2 Score:", r2_score(y_test, y_XGBpred))

def objective(params):
    # Convert certain parameters to integers
    params['max_depth'] = int(params['max_depth'])
    params['min_child_weight'] = int(params['min_child_weight'])
    params['reg_alpha'] = int(params['reg_alpha'])

    # Create the model with the given parameters
    model = XGBClassifier(
        max_depth=params['max_depth'],
        gamma=params['gamma'],
        reg_alpha=params['reg_alpha'],
        reg_lambda=params['reg_lambda'],
        colsample_bytree=params['colsample_bytree'],
        min_child_weight=params['min_child_weight'],
        n_estimators=params['n_estimators'],
        seed=params['seed']
    )

    # Perform cross-validation and return the negative AUC as the loss
    auc = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=3).mean()
    return {'loss': -auc, 'status': STATUS_OK}

# Define the hyperparameter space
space = {
    'max_depth': hp.quniform("max_depth", 3, 18, 1),
    'gamma': hp.uniform('gamma', 1, 9),
    'reg_alpha': hp.quniform('reg_alpha', 40, 180, 1),
    'reg_lambda': hp.uniform('reg_lambda', 0, 1),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),
    'min_child_weight': hp.quniform('min_child_weight', 0, 10, 1),
    'n_estimators': 180,
    'seed': 0
}

# Run hyperparameter optimization
trials = Trials()
best_params = fmin(
    fn=objective,  # Objective function
    space=space,  # Parameter space
    algo=tpe.suggest,  # Tree-structured Parzen Estimator (TPE)
    max_evals=50,  # Number of evaluations
    trials=trials,  # Store the trials
    rstate=np.random.default_rng(42)  # Seed for reproducibility
)

# Print the best parameters
print("Best parameters:", best_params)

model_Tun_XGB = XGBClassifier(colsample_bytree= 0.638387454538077, gamma= 1.0282196851358911, max_depth= 16, min_child_weight= 2, reg_alpha= 41, reg_lambda= 0.6116144221082778)

model_Tun_XGB.fit(X_train, y_train)

y_Tun_XGBpred = model_Tun_XGB.predict(X_test)
#y_svcproba = SVC_Model.predict_proba(X_test)[:, 1]
y_Tun_XGBproba = model_Tun_XGB.predict_proba(X_test)[:, 1]
# Evaluate the model
print("Confusion Matrix:\n", confusion_matrix(y_test, y_Tun_XGBpred))
print("\nClassification Report:\n", classification_report(y_test, y_Tun_XGBpred))
print("\nROC-AUC Score:", roc_auc_score(y_test, y_Tun_XGBproba))
print("R2 Score:", r2_score(y_test, y_Tun_XGBpred))

import pickle
filename = 'final_model.sav'
pickle.dump(SVCTun_Model , open(filename, 'wb'))

# some time later...

# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(X_test, y_test)
print(result,'% Acuuracy')

import joblib
joblib.dump(SVCTun_Model, 'model.pkl')

